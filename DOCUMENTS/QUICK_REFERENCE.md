# Quick Reference Guide - RAG System Improvements

## ğŸš€ Quick Start

### Run Evaluation
```bash
# Generate responses
python evaluation/run_generation.py

# Score responses
python evaluation/run_scoring.py
```

### Run Application
```bash
python chatbot.py
# or
python chatbot_refactored.py
```

---

## ğŸ“Š Key Metrics to Monitor

| Metric | Target | What It Measures |
|--------|--------|------------------|
| **Faithfulness** | > 0.8 | Answer grounded in context |
| **Hallucination Rate** | < 0.2 | Fabricated information |
| **Context Relevance** | > 0.7 | Quality of retrieval |
| **Answer Relevance** | > 0.85 | Addresses user query |
| **Precision** | > 0.8 | Correct claims |
| **Recall** | > 0.6 | Information coverage |

---

## ğŸ”§ Key Configuration Parameters

### Retrieval (`retriever/retriever.py`)
```python
k = 8                      # Number of documents to retrieve
similarity_threshold = 0.5  # Minimum similarity score
expansion_k = 12           # Expanded retrieval if needed
expansion_threshold = 0.4  # Lower threshold for expansion
```

### Generation (`generator/generator_llm.py`)
```python
max_new_tokens = 300       # Response length
temperature = 0.1          # Determinism (lower = more factual)
repetition_penalty = 1.1   # Reduce repetition
decoding_method = "greedy" # Most likely tokens
```

### Intent Classification (`context_expansion/intent_analyzer.py`)
```python
# Classifies queries as CLEAR or AMBIGUOUS
# AMBIGUOUS queries trigger follow-up questions
```

---

## ğŸ› Common Issues & Solutions

### Issue: Application Crashes
**Solution:** Ensure all dependencies are installed
```bash
pip install -r requirements.txt
```

### Issue: High Hallucination
**Solutions:**
1. Lower temperature (currently 0.1)
2. Strengthen prompt rules
3. Improve context quality
4. Increase similarity threshold

### Issue: Poor Context Retrieval
**Solutions:**
1. Adjust similarity threshold
2. Increase k value
3. Improve query preprocessing
4. Consider query expansion

### Issue: Truncated Responses
**Solution:** Increase `max_new_tokens` (currently 300)

---

## ğŸ“ File Structure

```
RAG-based-tech-support/
â”œâ”€â”€ chatbot.py                      # Main application
â”œâ”€â”€ chatbot_refactored.py           # Modular version
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ IMPROVEMENTS_SUMMARY.md         # Detailed improvements
â”œâ”€â”€ QUICK_REFERENCE.md              # This file
â”‚
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ retriever.py                # Context retrieval logic
â”‚   â””â”€â”€ vector_store.py             # Milvus connection
â”‚
â”œâ”€â”€ generator/
â”‚   â”œâ”€â”€ generator_llm.py            # LLM initialization
â”‚   â””â”€â”€ prompt_builder.py           # Prompt construction
â”‚
â”œâ”€â”€ context_expansion/
â”‚   â”œâ”€â”€ intent_analyzer.py          # Intent classification
â”‚   â”œâ”€â”€ intent_llm.py               # LLM for intent
â”‚   â””â”€â”€ intent_prompt.py            # Intent prompts
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ README.md                   # Evaluation guide
â”‚   â”œâ”€â”€ run_generation.py           # Generate responses
â”‚   â”œâ”€â”€ run_scoring.py              # Calculate metrics
â”‚   â”œâ”€â”€ results.json                # Generated responses
â”‚   â”œâ”€â”€ scored_results.json         # Scored results
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ eval_queries.json       # Test queries
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ context_relevance.py
â”‚   â”‚   â”œâ”€â”€ faithfulness.py
â”‚   â”‚   â”œâ”€â”€ answer_relevance.py
â”‚   â”‚   â””â”€â”€ precision_recall.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ llm_judge.py
â”‚       â”œâ”€â”€ claim_extractor.py
â”‚       â””â”€â”€ number_extractor.py
â”‚
â”œâ”€â”€ utils/                          # NEW
â”‚   â”œâ”€â”€ constants.py                # Configuration
â”‚   â””â”€â”€ helpers.py                  # Utility functions
â”‚
â”œâ”€â”€ ui/                             # NEW
â”‚   â””â”€â”€ gradio_interface.py         # UI components
â”‚
â””â”€â”€ data_prep/
    â”œâ”€â”€ data_loader.py              # Load dataset
    â”œâ”€â”€ data_embedding.py           # Create embeddings
    â”œâ”€â”€ insert_data.py              # Insert to Milvus
    â””â”€â”€ store_data.py               # Store management
```

---

## ğŸ¯ Improvement Checklist

### âœ… Completed
- [x] Fixed critical bugs (retrieve_context, intent classification)
- [x] Enhanced prompt engineering
- [x] Optimized generator parameters
- [x] Fixed evaluation metrics
- [x] Added comprehensive documentation
- [x] Refactored codebase

### ğŸ”„ In Progress
- [ ] Running evaluation pipeline
- [ ] Analyzing results

### ğŸ“‹ Pending
- [ ] Query expansion
- [ ] Reranking
- [ ] Response validation
- [ ] Confidence scoring
- [ ] Automated testing

---

## ğŸ’¡ Best Practices

### 1. Always Evaluate Before Changes
```bash
# Baseline
python evaluation/run_generation.py
python evaluation/run_scoring.py
cp evaluation/scored_results.json baseline_results.json
```

### 2. Make Small, Measurable Changes
- Change one parameter at a time
- Re-evaluate after each change
- Document what worked

### 3. Monitor Key Metrics
- Focus on faithfulness (hallucination)
- Track context relevance (retrieval quality)
- Measure answer relevance (user satisfaction)

### 4. Use Version Control
```bash
git add .
git commit -m "Improved prompt engineering - reduced hallucination by X%"
```

---

## ğŸ” Debugging Tips

### Check Vector Store Connection
```python
from retriever.vector_store import get_vectorstore
vectorstore = get_vectorstore()
print(f"Collection count: {vectorstore.col.num_entities}")
```

### Test Single Query
```python
from retriever.retriever import retrieve_context
context, similarity = retrieve_context(vectorstore, "test query")
print(f"Context: {context[:200]}...")
print(f"Similarity: {similarity}")
```

### Verify LLM Connection
```python
from generator.generator_llm import get_llm
llm = get_llm()
response = llm.generate(["Test prompt"])
print(response)
```

---

## ğŸ“ Support

### Documentation
- `README.md` - Project overview
- `evaluation/README.md` - Evaluation guide
- `IMPROVEMENTS_SUMMARY.md` - Detailed changes

### Key Concepts
- **RAG**: Retrieval-Augmented Generation
- **Hallucination**: Model generating false information
- **Faithfulness**: Answer grounded in context
- **Context Relevance**: Quality of retrieved documents

---

## ğŸ“ Learning Resources

### Understanding RAG
- Retrieval: Finding relevant documents
- Augmentation: Adding context to prompt
- Generation: LLM creates response

### Reducing Hallucination
1. **Prompt Engineering**: Clear instructions
2. **Temperature Control**: Lower = more factual
3. **Context Quality**: Better retrieval
4. **Response Validation**: Check grounding

### Evaluation Metrics
- **Precision**: Correctness of claims
- **Recall**: Coverage of information
- **Faithfulness**: Grounding in context
- **Relevance**: Addressing the query

---

## ğŸš¦ Status Indicators

### System Health
- âœ… **Green**: All metrics above targets
- âš ï¸ **Yellow**: Some metrics below targets
- ğŸ”´ **Red**: Critical metrics failing

### Current Status
- **Faithfulness**: ğŸ”„ Evaluating...
- **Context Relevance**: ğŸ”„ Evaluating...
- **Answer Relevance**: ğŸ”„ Evaluating...

---

## ğŸ“ˆ Next Steps

1. âœ… Complete evaluation run
2. ğŸ“Š Analyze results
3. ğŸ¯ Identify weak areas
4. ğŸ”§ Make targeted improvements
5. ğŸ”„ Re-evaluate
6. ğŸ“ Document findings

---

**Last Updated:** 2026-02-16  
**Version:** 2.0 (Post-improvements)